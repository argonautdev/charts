---
version: "v1"
appName: "nginx-deployment"
env: "dev"
hosts:
  - "app.tritonhq.io"
  - dev.tritonhq.io

argonaut:
  cloudProvider: aws
  cloudRegion: us-east-2
  cloudCluster: argonaut
  artDirGitURL: "https://github.com/argonautdev/art/" # Git URL that contains a ".art" folder which contains this "art.yaml" file
  gitBranch: main
  imageRegistry: ghcr.io # corresponding to the image that is to be deployed
  serviceType: "stateless" # One of {stateful, stateless, managed}

image: "nginx"
imageTag: "latest"

persistentStorage: # Set to [] if no persistent storage is required
  - capacity: "1000M"
    mountPath: "/usr/share/appdata"
    accessMode: "ReadWriteOnce" # ReadWriteOnce and ReadOnlyMany are supported

replicas: 1
minReplicas: 1
maxReplicas: 5

resources:
  requests:
    cpu: "100m"
    memory: "200M"
  # limits:
  #   cpu: "200m"
  #   memory: "256M"

# entrypoint: ["echo"] # overrides
# cmd: ["Hello World"] # overrides

services:
  - name: "80" # appname will be prefixed, needs to be unique
    protocol: http # http, tls, tcp
    port: 80 # number only
    ingress:
      enabled: true
      tls: "" # "terminated" or "" (not applicable) or "passthrough"
      port: 3000
      path: "/" # prefix regex match
  # - name: "443" # appname will be prefixed, needs to be unique
  #   protocol: tls # http, tls, tcp
  #   port: 443 # number only
  #   ingress:
  #     enabled: true
  #     tls: "terminated" # "terminated" or "" (not applicable) or "passthrough"
  #     port: 80
  #     path: "/" # prefix regex match

# Can only do one of the httpGet and exec handler methods for livenessProbe
livenessProbe:
  httpGet:
    path: /
    port: http
  # exec:
  #   command:
  #     - sh
  #     - -c
  #     - |
  #       #!/usr/bin/env sh
  #       test -f /etc/
  failureThreshold: 5
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5

# Can only do one of the httpGet and exec handler methods for readinessProbe
readinessProbe:
  # Handler 1
  httpGet:
    path: /
    port: 80
  # # Handler 2
  # exec:
  #   command:
  #     - sh
  #     - -c
  #     - |
  #       #!/usr/bin/env sh
  #       test -f /etc/
  # Common fields
  failureThreshold: 5
  initialDelaySeconds: 10
  successThreshold: 3
  periodSeconds: 10
  timeoutSeconds: 5

#########################################################################################
# Everything below this is optional and advanced configuration                          #
# and irrelevant in most scenarios.                                                     #
#########################################################################################

externalServices:
  []
  # - name: rds-postgres
  #   hosts: # Supports only one host
  #     - database-1.clo0rbqzxdpa.us-east-2.rds.amazonaws.com
  #   ports:
  #     - number: 5432
  #       name: postgresql
  #       protocol: TCP
  #   resolution: NONE # DNS
  #   location: MESH_EXTERNAL
  #   # tls: Disabled

podAnnotations:
  {}
  # iam.amazonaws.com/role: myapp-cluster

# additionals labels
labels: {}

# Allows you to load environment variables from kubernetes secret or config map
envFrom: []
# - secretRef:
#     name: env-secret
# - configMapRef:
#     name: config-map

# A list of secrets and their paths to mount inside the pod
# This is useful for mounting certificates for security
secretMounts: []
#  - name: myapp-certificates
#    secretName: myapp-certificates
#    path: /usr/share/myapp/config/certs
#    defaultMode: 0755

sidecarResources:
  {}
  # limits:
  #   cpu: "25m"
  #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

# networkHost: "0.0.0.0"

# The default value of 1 will make sure that kubernetes won't allow more than 1
# of your pods to be unavailable during maintenance
# maxUnavailable: 25%
updateStrategy: RollingUpdate
# How long to wait for myapp to stop gracefully
terminationGracePeriod: 30

lifecycle:
  {}
  # preStop:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  # postStart:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

rbac:
  create: false
  serviceAccountAnnotations: {}
  serviceAccountName: ""

podSecurityPolicy:
  create: false
  name: ""
  spec:
    privileged: true
    fsGroup:
      rule: RunAsAny
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
      - secret
      - configMap
      - persistentVolumeClaim

# This is the PriorityClass settings as defined in
# https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName: ""

# By default this will make sure two pods don't end up on the same node
# Changing this to a region would allow you to spread pods across regions
# This doesn't apply if antiAffinity is not set
antiAffinityTopologyKey: "kubernetes.io/hostname"

# "hard" means that by default pods will only be scheduled if there are enough nodes for them
# and that they will never end up on the same node. Setting this to "soft" will do this best effort
antiAffinity: ""

# This is the node affinity settings as defined in
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
nodeAffinity: {}

# The default is to deploy all pods serially. By setting this to parallel all pods are started at
# the same time when bootstrapping the cluster
podManagementPolicy: "Parallel"

podSecurityContext:
  {}
  # fsGroup: 1000
  # runAsUser: 1000

securityContext:
  {}
  # capabilities:
  #   drop:
  #     - ALL
  # # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

## Use an alternate scheduler.
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ""

nodeSelector: {}
tolerations: []

initContainer:
  enabled: false
  # command: ["echo", "I am an initContainer"]
  # image: nginx

initResources:
  {}
  # limits:
  #   cpu: "25m"
  #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

extraInitContainers:
  []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

extraVolumes:
  []
  # - name: extras
  #   emptyDir: {}

extraVolumeMounts:
  []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

extraContainers:
  []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

# Allows you to add any config files in /usr/share/myapp/config/
# as a ConfigMap
extraConfig:
  - name: configName
    path: "/path/to/config/folder/"
    readOnly: true
    data:
      pokedex.yaml: |
        pokemonName: Pikachu
        pokemonType: Lightning

      battle.yaml: |
        pokemon1: Pikachu
        pokemon2: MewTwo
  - name: configName2
    path: "/path/to/config/anotherfolder/"
    readOnly: true
    data:
      pokedex.yaml: |
        pokemonName: Pikachu
        pokemonType: Lightning

      battle.yaml: |
        pokemon1: Pikachu
        pokemon2: MewTwo

# Extra environment variables to append to this nodeGroup
# This will be appended to the current 'env:' key. You can use any of the kubernetes env
# syntax here
extraEnvs: []
#  - name: MY_ENVIRONMENT_VAR
#    value: the_value_goes_here
