{{- $name := include "clickhouse.name" . -}}
# We may need to label nodes with clickhouse=allow label for this example to run
# See ./label_nodes.sh for this purpose

apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: {{ $name }}
  labels:
    {{- if .Values.labels }}
    {{- toYaml .Values.labels | nindent 4 }}
    {{- end }}
  finalizers: []
    # - finalizer.clickhouseinstallation.altinity.com
  annotations:
    checksum/config: "{{- print .Values.clickhouse.configuration | sha256sum -}}"
    {{- if .Values.annotations }}
    {{- toYaml .Values.annotations | nindent 4 }}
    {{- end }}

spec:

  stop: "no"
  restart: "RollingUpdate"
  troubleshoot: "no"

  namespaceDomainPattern:  "%s.svc.my.test"

  templating:
    policy: "manual"

  reconciling:
    policy: "nowait"
    configMapPropagationTimeout: 90
    cleanup:
      unknownObjects:
        statefulSet: Delete
        pvc: Retain
        configMap: Delete
        service: Delete
      reconcileFailedObjects:
        statefulSet: Retain
        pvc: Retain
        configMap: Retain
        service: Retain

  # # List of templates used by a CHI
  # useTemplates:
  #   - name: template1
  #     namespace: ns1
  #     useType: merge
  #   - name: template2
  #     # No namespace specified - use CHI namespace

  defaults:
    replicasUseFQDN: "no"
    distributedDDL:
      profile: default
    templates:
      {{- with .Values.clickhouse.configuration }}
      {{- if .hostTemplates }}
      hostTemplate: {{ .hostTemplate | quote }}
      {{- else }}
      hostTemplate: "{{ $name }}-host-template-custom-ports"
      {{- end }}
      {{- if .dataVolumeClaimTemplate }}
      dataVolumeClaimTemplate: {{ .dataVolumeClaimTemplate | quote }}
      {{- else }}
      dataVolumeClaimTemplate: "{{ $name }}-data-volume-claim"
      {{- end }}
      {{- if .logVolumeClaimTemplate }}
      logVolumeClaimTemplate: {{ .logVolumeClaimTemplate | quote }}
      {{- else }}
      logVolumeClaimTemplate: "{{ $name }}-log-volume-claim"
      {{- end }}
      {{- if .serviceTemplate }}
      serviceTemplate: {{ .serviceTemplate | quote }}
      {{- else }}
      serviceTemplate: "{{ $name }}-default-svc-tmpl"
      {{- end }}
      {{- if .podTemplate }}
      podTemplate: {{ .podTemplate | quote }}
      {{- else }}
      podTemplate: "{{ $name }}-default-pod-tmpl"
      {{- end }}
      {{- if .clusterServiceTemplate }}
      clusterServiceTemplate: {{ .clusterServiceTemplate | quote }}
      {{- else }}
      clusterServiceTemplate: "{{ $name }}-default-svc-tmpl"
      {{- end }}
      {{- if .shardServiceTemplate }}
      shardServiceTemplate: {{ .shardServiceTemplate | quote }}
      {{- else }}
      shardServiceTemplate: "{{ $name }}-default-svc-tmpl"
      {{- end }}
      {{- if .replicaServiceTemplate }}
      replicaServiceTemplate: {{ .replicaServiceTemplate | quote }}
      {{- else }}
      replicaServiceTemplate: "{{ $name }}-default-svc-tmpl"
      {{- end }}
      {{- end }}

  configuration:
    users:
      admin/password: "{{ .Values.clickhouse.password }}"
      admin/networks/ip: "0.0.0.0/0"
      admin/profile: default
      admin/quota: default
      readonly/profile: readonly
      #     <users>
      #        <readonly>
      #          <profile>readonly</profile>
      #        </readonly>
      #     </users>
      test/networks/ip:
        - "127.0.0.1"
        - "::/0"
      #     <users>
      #        <test>
      #          <networks>
      #            <ip>127.0.0.1</ip>
      #            <ip>::/0</ip>
      #          </networks>
      #        </test>
      #     </users>
      test/profile: default
      test/quotas: default
    profiles:
      default/allow_experimental_window_functions: "1"
      readonly/readonly: "1"
      #      <profiles>
      #        <readonly>
      #          <readonly>1</readonly>
      #        </readonly>
      #      </profiles>
      # default/max_memory_usage: "3000000000"
    zookeeper:
      nodes:
        {{range until (.Values.zookeeper.nodes | int) }}
        - host: "{{ $name }}-{{ . }}-zk"
          port: 2181
        {{end}}
      session_timeout_ms: 30000
      operation_timeout_ms: 10000
      root: "/path/to/zookeeper/root/node"
      identity: "user:password"
    quotas:
      default/interval/duration: "3600"
      #     <quotas>
      #       <default>
      #         <interval>
      #           <duration>3600</duration>
      #         </interval>
      #       </default>
      #     </quotas>
    settings:
      compression/case/method: zstd
      #      <compression>
      #        <case>
      #          <method>zstd</method>
      #        </case>
      #      </compression>
      disable_internal_dns_cache: 1
      #      <disable_internal_dns_cache>1</disable_internal_dns_cache>
    {{- if .Values.clickhouse.configuration }}
    {{- with .Values.clickhouse.configuration.files }}
    files:
    {{- toYaml . | nindent 6 }}
    {{- end }}
    {{- end }}
    clusters:
      - name: '{{ $name | trunc 11 | trimSuffix "-" }}'
        layout:
          shardsCount: {{ .Values.clickhouse.shardsCount }}
          replicasCount: {{ .Values.clickhouse.replicasCount }}
  templates:
    hostTemplates:
      - name: "{{ $name }}-host-template-custom-ports"
        spec:
          tcpPort: 7000
          httpPort: 7001
          interserverHTTPPort: 7002
    serviceTemplates:
      {{- if .Values.clickhouse.configuration }}
      {{- if .Values.clickhouse.configuration.serviceTemplates }}
      {{- with .Values.clickhouse.configuration.serviceTemplates }}
      {{- toYaml . | nindent 6 }}
      {{- end }}
      {{- end }}
      {{- end }}
      - name: "{{ $name }}-default-svc-tmpl"
        # generateName understands different sets of macroses,
        # depending on the level of the object, for which Service is being created:
        #
        # For CHI-level Service:
        # 1. {chi} - ClickHouseInstallation name
        # 2. {chiID} - short hashed ClickHouseInstallation name (BEWARE, this is an experimental feature)
        #
        # For Cluster-level Service:
        # 1. {chi} - ClickHouseInstallation name
        # 2. {chiID} - short hashed ClickHouseInstallation name (BEWARE, this is an experimental feature)
        # 3. {cluster} - cluster name
        # 4. {clusterID} - short hashed cluster name (BEWARE, this is an experimental feature)
        # 5. {clusterIndex} - 0-based index of the cluster in the CHI (BEWARE, this is an experimental feature)
        #
        # For Shard-level Service:
        # 1. {chi} - ClickHouseInstallation name
        # 2. {chiID} - short hashed ClickHouseInstallation name (BEWARE, this is an experimental feature)
        # 3. {cluster} - cluster name
        # 4. {clusterID} - short hashed cluster name (BEWARE, this is an experimental feature)
        # 5. {clusterIndex} - 0-based index of the cluster in the CHI (BEWARE, this is an experimental feature)
        # 6. {shard} - shard name
        # 7. {shardID} - short hashed shard name (BEWARE, this is an experimental feature)
        # 8. {shardIndex} - 0-based index of the shard in the cluster (BEWARE, this is an experimental feature)
        #
        # For Replica-level Service:
        # 1. {chi} - ClickHouseInstallation name
        # 2. {chiID} - short hashed ClickHouseInstallation name (BEWARE, this is an experimental feature)
        # 3. {cluster} - cluster name
        # 4. {clusterID} - short hashed cluster name (BEWARE, this is an experimental feature)
        # 5. {clusterIndex} - 0-based index of the cluster in the CHI (BEWARE, this is an experimental feature)
        # 6. {shard} - shard name
        # 7. {shardID} - short hashed shard name (BEWARE, this is an experimental feature)
        # 8. {shardIndex} - 0-based index of the shard in the cluster (BEWARE, this is an experimental feature)
        # 9. {replica} - replica name
        # 10. {replicaID} - short hashed replica name (BEWARE, this is an experimental feature)
        # 11. {replicaIndex} - 0-based index of the replica in the shard (BEWARE, this is an experimental feature)
        # 12. {chiScopeIndex} - 0-based index of the host in the chi (BEWARE, this is an experimental feature)
        # 13. {chiScopeCycleIndex} - 0-based index of the host's cycle in the chi-scope (BEWARE, this is an experimental feature)
        # 14. {chiScopeCycleOffset} - 0-based offset of the host in the chi-scope cycle (BEWARE, this is an experimental feature)
        # 15. {clusterScopeIndex} - 0-based index of the host in the cluster (BEWARE, this is an experimental feature)
        # 16. {clusterScopeCycleIndex} - 0-based index of the host's cycle in the cluster-scope (BEWARE, this is an experimental feature)
        # 17. {clusterScopeCycleOffset} - 0-based offset of the host in the cluster-scope cycle (BEWARE, this is an experimental feature)
        # 18. {shardScopeIndex} - 0-based index of the host in the shard (BEWARE, this is an experimental feature)
        # 19. {replicaScopeIndex} - 0-based index of the host in the replica (BEWARE, this is an experimental feature)
        # 20. {clusterScopeCycleHeadPointsToPreviousCycleTail} - 0-based cluster-scope index of previous cycle tail
        generateName: "{{ $name }}"
        # type ObjectMeta struct from k8s.io/meta/v1
        metadata:
          labels:
            {{- if .Values.labels }}
            {{- toYaml .Values.labels | nindent 12 }}
            {{- end }}
          annotations:
            {{- if .Values.annotations }}
            {{- toYaml .Values.annotations | nindent 12 }}
            {{- end }}
            # For more details on Internal Load Balancer check
            # https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
            cloud.google.com/load-balancer-type: "Internal"
            service.beta.kubernetes.io/aws-load-balancer-internal: "true"
            service.beta.kubernetes.io/azure-load-balancer-internal: "true"
            service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
            service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"

            # NLB Load Balancer
            service.beta.kubernetes.io/aws-load-balancer-type: "nlb"

        # type ServiceSpec struct from k8s.io/core/v1
        spec:
          ports:
            - name: http
              port: 8123
            - name: tcp
              port: 9000
            - name: interserver
              port: 9009
          type: ClusterIP

    volumeClaimTemplates:
      {{- if .Values.clickhouse.configuration }}
      {{- if .Values.clickhouse.configuration.volumeClaimTemplates }}
      {{- with .Values.clickhouse.configuration.volumeClaimTemplates }}
      {{- toYaml . | nindent 6 }}
      {{- end }}
      {{- end }}
      {{- end }}
      - name: "{{ $name }}-data-volume-claim"
        # Keep PVC from being deleted
        # Retaining PVC will also keep backing PV from deletion. This is useful in case we need to keep data intact.
        reclaimPolicy: Retain
        # type PersistentVolumeClaimSpec struct from k8s.io/core/v1
        spec:
          # 1. If storageClassName is not specified, default StorageClass
          # (must be specified by cluster administrator) would be used for provisioning
          # 2. If storageClassName is set to an empty string (‘’), no storage class will be used
          # dynamic provisioning is disabled for this PVC. Existing, “Available”, PVs
          # (that do not have a specified storageClassName) will be considered for binding to the PVC
          #storageClassName: gold
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: "{{ .Values.clickhouse.storage }}"
      - name: "{{ $name }}-log-volume-claim"
        # Keep PVC from being deleted
        # Retaining PVC will also keep backing PV from deletion. This is useful in case we need to keep data intact.
        reclaimPolicy: Retain
        # type PersistentVolumeClaimSpec struct from k8s.io/core/v1
        spec:
          # 1. If storageClassName is not specified, default StorageClass
          # (must be specified by cluster administrator) would be used for provisioning
          # 2. If storageClassName is set to an empty string (‘’), no storage class will be used
          # dynamic provisioning is disabled for this PVC. Existing, “Available”, PVs
          # (that do not have a specified storageClassName) will be considered for binding to the PVC
          #storageClassName: gold
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: {{ .Values.clickhouse.logStorage | quote | default "5Gi" }}

    podTemplates:
      {{- if .Values.clickhouse.configuration }}
      {{- if .Values.clickhouse.configuration.podTemplates }}
      {{- with .Values.clickhouse.configuration.podTemplates }}
      {{- toYaml . | nindent 6 }}
      {{- end }}
      {{- end }}
      {{- end }}
      # multiple pod templates makes possible to update version smoothly
      # pod template for ClickHouse v18.16.1
      - name: {{ $name }}-default-pod-tmpl
        # We may need to label nodes with clickhouse=allow label for this example to run
        # See ./label_nodes.sh for this purpose
        # zone:
        #   key: "clickhouse"
        #   values:
        #     - "allow"
        # Shortcut version for AWS installations
        #zone:
        #  values:
        #    - "us-east-1a"

        # Possible values for podDistribution are:
        # Unspecified - empty value
        # ClickHouseAntiAffinity - AntiAffinity by ClickHouse instances.
        #   Pod pushes away other ClickHouse pods, which allows one ClickHouse instance per topologyKey-specified unit
        #   CH - (push away) - CH - (push away) - CH
        # ShardAntiAffinity - AntiAffinity by shard name.
        #   Pod pushes away other pods of the same shard (replicas of this shard),
        #   which allows one replica of a shard instance per topologyKey-specified unit.
        #   Other shards are allowed - it does not push all shards away, but CH-instances of the same shard only.
        #   Used for data loss avoidance - keeps all copies of the shard on different topologyKey-specified units.
        #   shard1,replica1 - (push away) - shard1,replica2 - (push away) - shard1,replica3
        # ReplicaAntiAffinity - AntiAffinity by replica name.
        #   Pod pushes away other pods of the same replica (shards of this replica),
        #   which allows one shard of a replica per topologyKey-specified unit.
        #   Other replicas are allowed - it does not push all replicas away, but CH-instances of the same replica only.
        #   Used to evenly distribute load from "full cluster scan" queries.
        #   shard1,replica1 - (push away) - shard2,replica1 - (push away) - shard3,replica1
        # AnotherNamespaceAntiAffinity - AntiAffinity by "another" namespace.
        #   Pod pushes away pods from another namespace, which allows same-namespace pods per topologyKey-specified unit.
        #   ns1 - (push away) - ns2 - (push away) - ns3
        # AnotherClickHouseInstallationAntiAffinity - AntiAffinity by "another" ClickHouseInstallation name.
        #   Pod pushes away pods from another ClickHouseInstallation,
        #   which allows same-ClickHouseInstallation pods per topologyKey-specified unit.
        #   CHI1 - (push away) - CHI2 - (push away) - CHI3
        # AnotherClusterAntiAffinity - AntiAffinity by "another" cluster name.
        #   Pod pushes away pods from another Cluster,
        #   which allows same-cluster pods per topologyKey-specified unit.
        #   cluster1 - (push away) - cluster2 - (push away) - cluster3
        # MaxNumberPerNode - AntiAffinity by cycle index.
        #   Pod pushes away pods from the same cycle,
        #   which allows to specify maximum number of ClickHouse instances per topologyKey-specified unit.
        #   Used to setup circular replication.
        # NamespaceAffinity - Affinity by namespace.
        #   Pod attracts pods from the same namespace,
        #   which allows pods from same namespace per topologyKey-specified unit.
        #   ns1 + (attracts) + ns1
        # ClickHouseInstallationAffinity - Affinity by ClickHouseInstallation name.
        #   Pod attracts pods from the same ClickHouseInstallation,
        #   which allows pods from the same CHI per topologyKey-specified unit.
        #   CHI1 + (attracts) + CHI1
        # ClusterAffinity - Affinity by cluster name.
        #   Pod attracts pods from the same cluster,
        #   which allows pods from the same Cluster per topologyKey-specified unit.
        #   cluster1 + (attracts) + cluster1
        # ShardAffinity - Affinity by shard name.
        #   Pod attracts pods from the same shard,
        #   which allows pods from the same Shard per topologyKey-specified unit.
        #   shard1 + (attracts) + shard1
        # ReplicaAffinity - Affinity by replica name.
        #   Pod attracts pods from the same replica,
        #   which allows pods from the same Replica per topologyKey-specified unit.
        #   replica1 + (attracts) + replica1
        # PreviousTailAffinity - Affinity to overlap cycles. Used to make cycle pod distribution
        #   cycle head + (attracts to) + previous cycle tail
        podDistribution:
          - type: ShardAntiAffinity
          - type: MaxNumberPerNode
            number: 2
            # Apply podDistribution on per-host basis
            topologyKey: "kubernetes.io/hostname"
            # Apply podDistribution on per-zone basis
            #topologyKey: "kubernetes.io/zone"

        # type PodSpec struct {} from k8s.io/core/v1
        spec:
          containers:
            - name: clickhouse
              volumeMounts:
                - name: {{ $name }}-volume-claim
                  mountPath: /var/lib/clickhouse
              image: clickhouse/clickhouse-server:22.3
              resources:
                requests:
                  memory: "{{ .Values.clickhouse.memory }}"
                  cpu: "{{ .Values.clickhouse.cpu }}"
                limits: {}
                  # memory: "2000Mi"
                  # cpu: "1500m"
                  